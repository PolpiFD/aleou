"""
Module d'extraction Firecrawl pour donn√©es h√¥teli√®res
Architecture parall√©lis√©e et robuste avec gestion d'erreurs avanc√©e
"""

import os
import asyncio
import aiohttp
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from pydantic import BaseModel, Field
import json
from pathlib import Path

try:
    from firecrawl import FirecrawlApp
except ImportError:
    print("‚ö†Ô∏è Firecrawl-py non install√©. Installez avec: pip install firecrawl-py")
    FirecrawlApp = None

from config.settings import settings


class HotelFirecrawlSchema(BaseModel):
    """Sch√©ma Pydantic pour extraction Firecrawl des donn√©es h√¥teli√®res"""
    
    # Donn√©es quantitatives principales
    capacite_max: Optional[int] = Field(None, description="Capacit√© maximale totale de l'h√¥tel pour √©v√©nements")
    nombre_chambre: Optional[int] = Field(None, description="Nombre total de chambres dans l'h√¥tel")
    nombre_chambre_twin: Optional[int] = Field(None, description="Nombre de chambres twin/doubles")
    nombre_etoile: Optional[int] = Field(None, description="Nombre d'√©toiles de l'h√¥tel (1-5)")
    
    # Capacit√©s sp√©cialis√©es pour √©v√©nements d'affaires
    pr_amphi: Optional[int] = Field(None, description="Capacit√© amphith√©√¢tre/auditorium")
    pr_hotel: Optional[int] = Field(None, description="Capacit√© g√©n√©rale h√¥tel pour √©v√©nements")
    pr_acces_facile: Optional[int] = Field(None, description="Capacit√© espaces √† acc√®s facile/PMR")
    pr_banquet: Optional[int] = Field(None, description="Capacit√© banquet/d√Æner assis")
    pr_contact: Optional[int] = Field(None, description="Capacit√© espaces de networking/contact")
    pr_room_nb: Optional[int] = Field(None, description="Nombre total de salles de r√©union/√©v√©nement")
    
    # Caract√©ristiques bool√©ennes (Yes/No)
    pr_lieu_atypique: Optional[str] = Field(None, description="Lieu atypique/original (Yes/No)")
    pr_nature: Optional[str] = Field(None, description="Proximit√© nature/espaces verts (Yes/No)")
    pr_mer: Optional[str] = Field(None, description="Proximit√© mer/oc√©an (Yes/No)")
    pr_montagne: Optional[str] = Field(None, description="Proximit√© montagne (Yes/No)")
    pr_centre_ville: Optional[str] = Field(None, description="Localisation centre-ville (Yes/No)")
    pr_parking: Optional[str] = Field(None, description="Parking disponible (Yes/No)")
    pr_restaurant: Optional[str] = Field(None, description="Restaurant sur place (Yes/No)")
    pr_piscine: Optional[str] = Field(None, description="Piscine disponible (Yes/No)")
    pr_spa: Optional[str] = Field(None, description="Spa/bien-√™tre disponible (Yes/No)")
    pr_wifi: Optional[str] = Field(None, description="WiFi gratuit disponible (Yes/No)")
    pr_sun: Optional[str] = Field(None, description="Espaces ensoleill√©s/terrasses (Yes/No)")
    pr_contemporaine: Optional[str] = Field(None, description="Architecture contemporaine (Yes/No)")
    pr_acces_pmr: Optional[str] = Field(None, description="Acc√®s PMR/handicap√©s (Yes/No)")
    pr_visio: Optional[str] = Field(None, description="√âquipements visioconf√©rence (Yes/No)")
    pr_eco_label: Optional[str] = Field(None, description="Label √©cologique/d√©veloppement durable (Yes/No)")
    pr_rooftop: Optional[str] = Field(None, description="Rooftop/toit-terrasse disponible (Yes/No)")
    pr_esat: Optional[str] = Field(None, description="Partenariat ESAT/insertion (Yes/No)")
    
    # R√©sum√© et m√©tadonn√©es
    summary: Optional[str] = Field(None, description="R√©sum√© descriptif de l'h√¥tel en fran√ßais (150 mots max, optimis√© SEO, focus √©v√©nementiel/business)")
    
    # Nouvelles donn√©es enrichies Firecrawl
    hotel_website_title: Optional[str] = Field(None, description="Titre officiel du site web")
    hotel_phone: Optional[str] = Field(None, description="Num√©ro de t√©l√©phone principal")
    hotel_email: Optional[str] = Field(None, description="Email de contact principal")
    opening_hours: Optional[str] = Field(None, description="Horaires d'ouverture")
    price_range: Optional[str] = Field(None, description="Gamme de prix (‚Ç¨, $$, etc.)")
    
    # Donn√©es salles de r√©union (nouveau!)
    meeting_rooms_available: Optional[bool] = Field(None, description="Salles de r√©union disponibles")
    meeting_rooms_count: Optional[int] = Field(None, description="Nombre de salles de r√©union")
    largest_room_capacity: Optional[int] = Field(None, description="Capacit√© de la plus grande salle")

    # Images du site
    photos_urls: Optional[List[str]] = Field(default_factory=list, description="URLs des photos de l'h√¥tel")
    photos_count: Optional[int] = Field(None, description="Nombre de photos collect√©es")


@dataclass
class FirecrawlConfig:
    """Configuration pour Firecrawl"""
    api_key: str
    batch_size: int = 10  # Batch size = rate limit (10 req/min)
    max_concurrent_batches: int = 3
    timeout: int = 120
    formats: List[str] = None
    only_main_content: bool = True
    include_tags: List[str] = None
    exclude_tags: List[str] = None
    rate_limit_requests_per_minute: int = 10  # Plan gratuit = 10 req/min
    rate_limit_wait_seconds: int = 65  # Attendre 65s apr√®s chaque batch de 10
    
    def __post_init__(self):
        if self.formats is None:
            self.formats = ['markdown', 'html']
        if self.include_tags is None:
            self.include_tags = ['p', 'h1', 'h2', 'h3', 'div', 'span', 'ul', 'li']
        if self.exclude_tags is None:
            self.exclude_tags = ['script', 'style', 'nav', 'footer', 'ads']
        
        # Ajuster batch_size au rate limit pour parall√©lisme optimal
        if self.batch_size > self.rate_limit_requests_per_minute:
            self.batch_size = self.rate_limit_requests_per_minute


class FirecrawlExtractor:
    """Extracteur Firecrawl avec architecture parall√©lis√©e et robuste"""
    
    def __init__(self, config: FirecrawlConfig = None):
        if FirecrawlApp is None:
            raise ImportError("Firecrawl-py requis. Installez avec: pip install firecrawl-py")
        
        # Configuration
        self.config = config or FirecrawlConfig(
            api_key=os.getenv('FIRECRAWL_API_KEY', ''),
            rate_limit_requests_per_minute=int(os.getenv('FIRECRAWL_RATE_LIMIT', '10')),
            rate_limit_wait_seconds=int(os.getenv('FIRECRAWL_WAIT_SECONDS', '65'))
        )
        
        if not self.config.api_key:
            raise ValueError("FIRECRAWL_API_KEY manquant dans .env")
        
        # Client Firecrawl
        self.app = FirecrawlApp(api_key=self.config.api_key)
        
        # Sch√©ma d'extraction
        self.extraction_schema = HotelFirecrawlSchema.model_json_schema()
        
        # Session HTTP pour tracking des jobs
        self.session = None
        
        # Statistiques
        self.stats = {
            'total_urls': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'total_processing_time': 0.0,
            'start_time': None
        }
    
    async def __aenter__(self):
        """Context manager entry"""
        self.session = aiohttp.ClientSession()
        self.stats['start_time'] = datetime.now()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        if self.session:
            await self.session.close()
        
        # Log final stats
        if self.stats['start_time']:
            total_time = (datetime.now() - self.stats['start_time']).total_seconds()
            self.stats['total_processing_time'] = total_time
            print(f"üéØ FIRECRAWL STATS FINALES:")
            print(f"   üìä Total URLs: {self.stats['total_urls']}")
            print(f"   ‚úÖ Succ√®s: {self.stats['successful_extractions']}")
            print(f"   ‚ùå √âchecs: {self.stats['failed_extractions']}")
            print(f"   ‚è±Ô∏è Temps total: {total_time:.1f}s")
            if self.stats['successful_extractions'] > 0:
                avg_time = total_time / self.stats['successful_extractions']
                print(f"   ‚ö° Temps moyen/URL: {avg_time:.1f}s")
    
    async def extract_hotels_batch(self, hotel_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extrait les donn√©es pour un batch d'h√¥tels en parall√®le
        
        Args:
            hotel_data: Liste de donn√©es h√¥tels avec 'name', 'address', 'website_url'
            
        Returns:
            Liste des r√©sultats d'extraction
        """
        
        print(f"üöÄ D√©but extraction Firecrawl pour {len(hotel_data)} h√¥tels")
        self.stats['total_urls'] = len(hotel_data)
        
        # Diviser en batches selon rate limit (10 req/min = 10 URLs par batch)
        batches = self._create_batches(hotel_data, self.config.rate_limit_requests_per_minute)
        print(f"üì¶ {len(batches)} batches cr√©√©s (taille: {self.config.rate_limit_requests_per_minute} = rate limit)")
        
        # üö¶ TRAITEMENT S√âQUENTIEL des batches avec attente entre eux
        print(f"üö¶ Mode s√©quentiel avec {self.config.rate_limit_wait_seconds}s d'attente entre batches")
        
        all_batch_results = []
        for i, batch in enumerate(batches):
            if i > 0:  # Attendre apr√®s chaque batch sauf le premier
                print(f"‚è≥ Attente {self.config.rate_limit_wait_seconds}s avant batch {i+1}")
                await asyncio.sleep(self.config.rate_limit_wait_seconds)
            
            # Traiter le batch en parall√®le interne (jusqu'√† 10 requ√™tes simultan√©es)
            batch_result = await self._process_batch_parallel(batch, i+1)
            all_batch_results.append(batch_result)
        
        # Consolider tous les r√©sultats
        all_results = []
        for batch_result in all_batch_results:
            if isinstance(batch_result, Exception):
                print(f"‚ùå Erreur batch: {batch_result}")
                continue
            all_results.extend(batch_result)
        
        return all_results
    
    async def _process_batch_parallel(self, hotel_batch: List[Dict], batch_num: int) -> List[Dict]:
        """Traite un batch d'h√¥tels"""
        
        print(f"üì¶ Traitement batch {batch_num} ({len(hotel_batch)} h√¥tels) - PARALL√àLE interne")
        
        # Pr√©parer les URLs pour Firecrawl
        urls = []
        url_to_hotel = {}
        
        for hotel in hotel_batch:
            website_url = hotel.get('website_url', '')
            if website_url and website_url.startswith(('http://', 'https://')):
                urls.append(website_url)
                url_to_hotel[website_url] = hotel
        
        if not urls:
            print(f"‚ö†Ô∏è Batch {batch_num}: Aucune URL valide trouv√©e")
            return []
        
        try:
            # Appel Firecrawl batch scrape avec extraction
            batch_result = await self._firecrawl_batch_scrape(urls, batch_num)
            
            # Traiter les r√©sultats
            processed_results = []
            if batch_result and hasattr(batch_result, 'items'):
                for url, result in batch_result.items():
                    hotel_info = url_to_hotel.get(url, {})
                    processed_result = self._process_extraction_result(
                        hotel_info, url, result, batch_num
                    )
                    processed_results.append(processed_result)
            else:
                raise Exception("batch_result invalide - pas d'attribut items")
            
            success_count = sum(1 for r in processed_results if r['success'])
            print(f"‚úÖ Batch {batch_num}: {success_count}/{len(processed_results)} succ√®s")
            
            return processed_results
            
        except Exception as e:
            print(f"‚ùå Erreur batch {batch_num}: {e}")
            # Retourner des r√©sultats d'√©chec pour chaque h√¥tel
            return [self._create_failure_result(hotel, str(e)) for hotel in hotel_batch]
    
    async def _firecrawl_batch_scrape(self, urls: List[str], batch_num: int) -> Dict[str, Any]:
        """Effectue le scraping Firecrawl en mode batch"""
        
        print(f"üî• Firecrawl batch {batch_num}: scraping {len(urls)} URLs...")
        
        try:
            # üîß CORRECTION: Configuration simplifi√©e pour API v1
            # Plus besoin de configurations s√©par√©es avec la nouvelle API
            
            # Appel API Firecrawl
            # Note: Firecrawl batch peut prendre du temps, nous simulons ici
            results = {}
            
            # En mode r√©el, utiliser:
            # batch_job = self.app.batch_scrape_urls(urls, scrape_config)
            # results = self.app.get_batch_job_status(batch_job['jobId'])
            
            # üîß Utilisation de la m√©thode extract() pour extraction structur√©e
            print(f"üîß Extraction structur√©e pour {len(urls)} URLs...")
            
            # üö¶ TRAITEMENT S√âQUENTIEL: Rate limiting pour plan gratuit Firecrawl
            print(f"üö¶ Traitement s√©quentiel de {len(urls)} URLs avec rate limiting...")
            
            async def scrape_single_url(url):
                """Scrape une seule URL de mani√®re asynchrone (rate limiting g√©r√© au niveau sup√©rieur)"""
                try:
                    # Rate limiting g√©r√© dans la boucle s√©quentielle principale
                    
                    print(f"üéØ Extraction structur√©e URL: {url}")
                    # üéØ EXTRACTION STRUCTUR√âE avec syntaxe correcte Firecrawl
                    loop = asyncio.get_event_loop()
                    # ‚úÖ VRAIE SYNTAXE FIRECRAWL 2025 selon documentation officielle
                    result = await loop.run_in_executor(
                        None, 
                        lambda: self.app.extract(
                            urls=[url],  # Liste d'URLs (obligatoire)
                            prompt=self._build_extraction_prompt(),
                            schema=self.extraction_schema
                        )
                    )
                    
                    if result is None:
                        print(f"üî¥ R√âSULTAT NULL d√©tect√© pour {url}")
                        return url, {'error': 'Extract result is None'}
                    
                    # üéØ TRAITEMENT R√âSULTAT EXTRACT - ExtractResponse de Firecrawl
                    if hasattr(result, '__dict__'):
                        result_dict = result.__dict__
                        print(f"‚úÖ Extraction structur√©e r√©ussie pour {url}")
                        print(f"üîß ExtractResponse keys: {list(result_dict.keys())}")
                    elif isinstance(result, list) and len(result) > 0:
                        extract_data = result[0]
                        result_dict = extract_data.__dict__ if hasattr(extract_data, '__dict__') else extract_data
                        print(f"‚úÖ Extraction structur√©e r√©ussie pour {url}")
                    elif isinstance(result, dict):
                        result_dict = result
                        print(f"‚úÖ Extraction structur√©e r√©ussie pour {url}")
                    else:
                        print(f"‚ö†Ô∏è Format inattendu pour {url}: {type(result)}")
                        result_dict = {'error': f'Format inattendu: {type(result)}', 'raw_data': str(result)}

                    # R√©cup√©rer aussi des images du site
                    images = await self._scrape_images(url)
                    result_dict['photos_urls'] = images
                    result_dict['photos_count'] = len(images)
                    if isinstance(result_dict.get('data'), dict):
                        result_dict['data']['photos_urls'] = images
                        result_dict['data']['photos_count'] = len(images)

                    return url, result_dict
                    
                except Exception as e:
                    print(f"‚ùå Erreur scraping {url}: {e}")
                    return url, {'error': str(e)}
            
            # üöÄ PARALL√âLISME RESTAUR√â dans le batch (max 10 URLs simultan√©es)
            print(f"üöÄ Traitement parall√®le de {len(urls)} URLs (batch rate-limited)")
            
            scrape_tasks = [scrape_single_url(url) for url in urls]
            parallel_results = await asyncio.gather(*scrape_tasks, return_exceptions=True)
            
            # Assembler les r√©sultats
            results = {}
            for result in parallel_results:
                if isinstance(result, Exception):
                    print(f"‚ùå Exception dans task parall√®le: {result}")
                    continue
                    
                url, data = result
                results[url] = data
            
            print(f"üöÄ Parall√©lisme termin√©: {len(results)}/{len(urls)} URLs trait√©es")
            return results
                
        except Exception as batch_error:
            print(f"‚ùå √âchec extraction batch: {batch_error}")
            # Retourner des r√©sultats d'erreur pour toutes les URLs
            for url in urls:
                results[url] = {'error': f'Extraction batch √©chou√©e: {batch_error}'}
            return results
            
        except Exception as e:
            print(f"‚ùå Erreur Firecrawl batch {batch_num}: {e}")
            raise
    
    async def _extract_single_url_structured(self, url: str) -> Dict[str, Any]:
        """Extrait une seule URL avec la m√©thode extract structur√©e
        
        Args:
            url (str): URL √† extraire
            
        Returns:
            Dict[str, Any]: R√©sultat d'extraction 
        """
        
        try:
            # Rate limiting respectueux pour Firecrawl
            await asyncio.sleep(0.5)  # 500ms entre chaque URL
            
            # Utiliser extract() pour extraction structur√©e
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: self.app.extract(
                    urls=[url],  # Liste d'URLs (obligatoire)
                    prompt=self._build_extraction_prompt(),
                    schema=self.extraction_schema
                )
            )
            
            if hasattr(result, '__dict__'):
                result_dict = result.__dict__
                print(f"   ‚úÖ Extraction structur√©e r√©ussie")
                return result_dict
            else:
                return result if isinstance(result, dict) else {'data': result}
                
        except Exception as e:
            print(f"‚ùå Erreur extraction {url}: {e}")
            return {'error': f'Extraction √©chou√©e: {e}'}

    async def _scrape_images(self, url: str, max_images: int = 15) -> List[str]:
        """Scrape la page pour r√©cup√©rer des URLs d'images pertinentes"""

        loop = asyncio.get_event_loop()
        try:
            scrape_result = await loop.run_in_executor(
                None,
                lambda: self.app.scrape_url(url, formats=['html'])
            )

            html = ''
            if hasattr(scrape_result, 'html') and scrape_result.html:
                html = scrape_result.html
            elif isinstance(scrape_result, dict):
                html = scrape_result.get('html') or scrape_result.get('rawHtml', '')

            if not html:
                return []

            from bs4 import BeautifulSoup
            from urllib.parse import urljoin

            soup = BeautifulSoup(html, 'html.parser')
            images = []
            for img in soup.find_all('img'):
                src = img.get('src')
                alt = (img.get('alt') or '').lower()
                if not src:
                    continue
                if any(keyword in src.lower() for keyword in ['logo', 'icon']) or 'logo' in alt or 'icon' in alt:
                    continue
                if src.startswith('//'):
                    src = 'https:' + src
                if src.startswith('/'):
                    src = urljoin(url, src)
                if src.startswith('data:'):
                    continue
                if src not in images:
                    images.append(src)
                if len(images) >= max_images:
                    break

            return images

        except Exception as e:
            print(f"‚ùå Erreur r√©cup√©ration images {url}: {e}")
            return []
    
    def _build_extraction_prompt(self) -> str:
        """Construit le prompt d'extraction optimis√© pour les h√¥tels"""
        
        return """Tu es un expert en analyse de sites web d'h√¥tels et centres de congr√®s.

MISSION: Extraire les informations pr√©cises sur cet √©tablissement selon le sch√©ma JSON fourni.

PRIORIT√âS D'EXTRACTION:
1. Capacit√©s et nombre de chambres (chercher dans sections "H√©bergement", "Rooms", "Chambres")
2. √âquipements et services (parking, restaurant, piscine, spa, wifi, etc.)
3. Informations sur les salles de r√©union/√©v√©nements ("Meeting rooms", "Espaces √©v√©nementiels", "S√©minaires")
4. Localisation et caract√©ristiques (centre-ville, nature, mer, montagne)
5. Donn√©es de contact (t√©l√©phone, email, horaires)

INSTRUCTIONS SP√âCIALES:
- Pour les champs Yes/No: utilise exactement "Yes" ou "No" (jamais "Oui"/"Non")
- Pour les nombres: extrait uniquement les chiffres (ex: "150 chambres" ‚Üí 150)
- Pour le r√©sum√©: maximum 150 mots, focus sur capacit√©s √©v√©nementielles
- Ignore les donn√©es de navigation, cookies, menus

Si une information n'est pas trouv√©e, laisse le champ √† null."""
    
    def _process_extraction_result(self, hotel_info: Dict, url: str, firecrawl_result: Dict, batch_num: int) -> Dict[str, Any]:
        """Traite le r√©sultat d'extraction Firecrawl pour un h√¥tel"""
        
        hotel_name = hotel_info.get('name', 'Hotel_Unknown')
        
        try:
            # V√©rification basique du r√©sultat
            if firecrawl_result is None:
                raise Exception("firecrawl_result est None")
            
            # V√©rifier si l'extraction a r√©ussi
            if 'error' in firecrawl_result and firecrawl_result['error'] is not None:
                raise Exception(f"Erreur Firecrawl: {firecrawl_result['error']}")
            
            # üéØ TRAITEMENT R√âSULTAT EXTRACT() - Nouveau format structur√©
            extracted_data = None
            content_length = 0
            
            # Cas 1: ExtractResponse converti en dict
            if isinstance(firecrawl_result, dict) and 'data' in firecrawl_result:
                print(f"üéØ ExtractResponse d√©tect√© pour {hotel_name}")
                # Les donn√©es structur√©es sont dans le champ 'data'
                raw_extract_data = firecrawl_result.get('data', {})
                if isinstance(raw_extract_data, list) and len(raw_extract_data) > 0:
                    extracted_data = raw_extract_data[0]  # Premier √©l√©ment de la liste
                else:
                    extracted_data = raw_extract_data
                content_length = 0
                print(f"üîß Donn√©es extraites type: {type(extracted_data)}")
                
            # Cas 2: Donn√©es extract() directes 
            elif isinstance(firecrawl_result, dict) and any(key in ['capacite_max', 'nombre_chambre', 'summary', 'pr_parking'] for key in firecrawl_result.keys()):
                print(f"üéØ Donn√©es extract() directes d√©tect√©es pour {hotel_name}")
                extracted_data = firecrawl_result
                content_length = 0
                
            # Cas 3: Erreur dans extract()
            elif isinstance(firecrawl_result, dict) and 'error' in firecrawl_result:
                raise Exception(f"Erreur Firecrawl extract: {firecrawl_result['error']}")
            
            # Cas 4: R√©sultat vide ou format inattendu
            elif isinstance(firecrawl_result, dict) and len(firecrawl_result) == 0:
                print(f"‚ö†Ô∏è R√©sultat vide pour {hotel_name}")
                extracted_data = {}
            
            else:
                raise Exception(f"Format de r√©ponse Firecrawl inattendu: {type(firecrawl_result)} - Keys: {list(firecrawl_result.keys()) if isinstance(firecrawl_result, dict) else 'N/A'}")
            
            # Validation et nettoyage des donn√©es extraites
            validated_data = self._validate_extracted_data(extracted_data) if extracted_data else {}
            
            # M√©tadonn√©es enrichies
            metadata = {
                'url': url,
                'title': firecrawl_result.get('title', '') if isinstance(firecrawl_result, dict) else '',
                'description': firecrawl_result.get('description', '') if isinstance(firecrawl_result, dict) else '',
                'content_length': content_length,
                'extraction_method': 'firecrawl_extract',
                'batch_number': batch_num,
                'processing_date': datetime.now().isoformat(),
                'firecrawl_metadata': firecrawl_result.get('metadata', {}) if isinstance(firecrawl_result, dict) else {}
            }
            
            # R√©sultat de succ√®s
            result = {
                'success': True,
                'hotel_name': hotel_name,
                'hotel_address': hotel_info.get('address', ''),
                'website_data': validated_data,
                'metadata': metadata,
                'error': None
            }
            
            self.stats['successful_extractions'] += 1
            
            # Log pour extraction r√©ussie
            if validated_data and isinstance(validated_data, dict):
                try:
                    fields_count = len([k for k, v in validated_data.items() if v is not None])
                    print(f"‚úÖ {hotel_name}: extraction structur√©e r√©ussie ({fields_count} champs)")
                except Exception as items_error:
                    print(f"‚úÖ {hotel_name}: extraction structur√©e r√©ussie")
            else:
                print(f"‚úÖ {hotel_name}: extraction r√©ussie")
            
            return result
            
        except Exception as e:
            self.stats['failed_extractions'] += 1
            print(f"‚ùå {hotel_name}: {e}")
            return self._create_failure_result(hotel_info, str(e))
    
    
    def _validate_extracted_data(self, raw_data: Dict) -> Dict[str, Any]:
        """Valide et nettoie les donn√©es extraites par Firecrawl"""
        
        if not raw_data or not isinstance(raw_data, dict):
            return {}
        
        validated = {}
        
        # Valider les champs num√©riques
        numeric_fields = [
            'capacite_max', 'nombre_chambre', 'nombre_chambre_twin', 'nombre_etoile',
            'pr_amphi', 'pr_hotel', 'pr_acces_facile', 'pr_banquet', 
            'pr_contact', 'pr_room_nb', 'meeting_rooms_count', 'largest_room_capacity'
        ]
        
        for field in numeric_fields:
            value = raw_data.get(field)
            if value is not None:
                try:
                    validated[field] = int(value) if str(value).isdigit() else None
                except (ValueError, TypeError):
                    validated[field] = None
            else:
                validated[field] = None
        
        # Valider les champs Yes/No
        yes_no_fields = [
            'pr_lieu_atypique', 'pr_nature', 'pr_mer', 'pr_montagne', 'pr_centre_ville',
            'pr_parking', 'pr_restaurant', 'pr_piscine', 'pr_spa', 'pr_wifi', 'pr_sun',
            'pr_contemporaine', 'pr_acces_pmr', 'pr_visio', 'pr_eco_label', 'pr_rooftop', 'pr_esat'
        ]
        
        for field in yes_no_fields:
            value = raw_data.get(field)
            if value and isinstance(value, str):
                cleaned_value = value.strip().lower()
                if cleaned_value in ['yes', 'oui', 'true', '1', 'disponible', 'pr√©sent']:
                    validated[field] = 'Yes'
                elif cleaned_value in ['no', 'non', 'false', '0', 'indisponible', 'absent']:
                    validated[field] = 'No'
                else:
                    validated[field] = None
            else:
                validated[field] = None
        
        # Valider les champs bool√©ens
        boolean_fields = ['meeting_rooms_available']
        for field in boolean_fields:
            value = raw_data.get(field)
            if value is not None:
                validated[field] = bool(value)
            else:
                validated[field] = None
        
        # Valider les champs texte
        text_fields = [
            'summary', 'hotel_website_title', 'hotel_phone', 'hotel_email', 
            'opening_hours', 'price_range'
        ]
        
        for field in text_fields:
            value = raw_data.get(field)
            if value and isinstance(value, str):
                cleaned_value = value.strip()
                if field == 'summary' and len(cleaned_value) > 500:
                    # Limiter le r√©sum√©
                    words = cleaned_value.split()[:150]
                    validated[field] = ' '.join(words) + ('...' if len(words) == 150 else '')
                else:
                    validated[field] = cleaned_value
            else:
                validated[field] = None if field != 'summary' else ""

        # Images
        photos = raw_data.get('photos_urls', [])
        if isinstance(photos, list):
            validated['photos_urls'] = photos[:15]
            validated['photos_count'] = len(photos[:15])
        else:
            validated['photos_urls'] = []
            validated['photos_count'] = raw_data.get('photos_count', 0) or 0

        return validated
    
    def _create_failure_result(self, hotel_info: Dict, error_message: str) -> Dict[str, Any]:
        """Cr√©e un r√©sultat d'√©chec standardis√©"""
        
        return {
            'success': False,
            'hotel_name': hotel_info.get('name', 'Hotel_Unknown'),
            'hotel_address': hotel_info.get('address', ''),
            'website_data': {
                'photos_urls': [],
                'photos_count': 0
            },
            'metadata': {
                'url': hotel_info.get('website_url', ''),
                'extraction_method': 'firecrawl',
                'processing_date': datetime.now().isoformat()
            },
            'error': error_message
        }
    
    def _create_batches(self, data: List[Any], batch_size: int) -> List[List[Any]]:
        """Divise les donn√©es en batches"""
        return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]


# Fonction d'usage pratique
async def extract_hotels_with_firecrawl(hotels_data: List[Dict[str, Any]], 
                                       output_dir: str = "outputs") -> List[Dict[str, Any]]:
    """Fonction utilitaire pour extraire des donn√©es d'h√¥tels avec Firecrawl
    
    Args:
        hotels_data: Liste de dictionnaires avec name, address, website_url
        output_dir: Dossier de sortie pour logs
        
    Returns:
        Liste des r√©sultats d'extraction
    """
    
    config = FirecrawlConfig(
        api_key=os.getenv('FIRECRAWL_API_KEY', ''),
        rate_limit_requests_per_minute=int(os.getenv('FIRECRAWL_RATE_LIMIT', '10')),
        rate_limit_wait_seconds=int(os.getenv('FIRECRAWL_WAIT_SECONDS', '65'))
    )
    
    async with FirecrawlExtractor(config) as extractor:
        results = await extractor.extract_hotels_batch(hotels_data)
    
    # Sauvegarder les r√©sultats
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path(output_dir) / f"firecrawl_extraction_{timestamp}.json"
    
    Path(output_dir).mkdir(exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"üìÑ R√©sultats sauvegard√©s: {output_file}")
    
    return results